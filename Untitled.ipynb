{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a81386-8592-4052-ab69-0e855d0e67c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_ReLU_64-32: best val acc = 0.9700\n",
      "MLP_ReLU_128-64-32 + Dropout: best val acc = 0.9735\n",
      "MLP_LeakyReLU_64-32 + BN: best val acc = 0.9760\n",
      "MLP_ELU_128-64 + Dropout+BN: best val acc = 0.9720\n",
      "\n",
      "Classes (Failure Type):\n",
      "['Heat Dissipation Failure', 'No Failure', 'Overstrain Failure', 'Power Failure', 'Random Failures', 'Tool Wear Failure']\n",
      "\n",
      "Best architecture:\n",
      "MLP_LeakyReLU_64-32 + BN with best val acc = 0.9760\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Data preparation\n",
    "# -----------------------------\n",
    "def prepare_failure_type_data(df: pd.DataFrame):\n",
    "    # Drop ID columns\n",
    "    df = df.drop(columns=[\"UDI\", \"Product ID\"], errors=\"ignore\")\n",
    "\n",
    "    # IMPORTANT: drop Target if it exists (leak)\n",
    "    df = df.drop(columns=[\"Target\"], errors=\"ignore\")\n",
    "\n",
    "    if \"Failure Type\" not in df.columns:\n",
    "        raise ValueError(\"Column 'Failure Type' not found in df.\")\n",
    "\n",
    "    X = df.drop(columns=[\"Failure Type\"])\n",
    "    y_raw = df[\"Failure Type\"].astype(str).values\n",
    "\n",
    "    # Encode y -> class indices\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "    # Split FIRST (no leakage)\n",
    "    X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Preprocessor (fit on train only)\n",
    "    cat_cols = [\"Type\"]\n",
    "    try:\n",
    "        ohe = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\")\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(drop=\"first\", sparse=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"cat\", ohe, cat_cols)],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train_df)\n",
    "    X_val = preprocessor.transform(X_val_df)\n",
    "\n",
    "    # Scale (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, preprocessor, scaler, label_encoder\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) PyTorch Dataset\n",
    "# -----------------------------\n",
    "class MaintenanceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multiclass Dataset:\n",
    "    - X: float32 tensor\n",
    "    - y: long tensor (class indices), shape (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)  # for CrossEntropyLoss\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Build model (multiple architectures)\n",
    "# -----------------------------\n",
    "def build_model(input_dim: int,\n",
    "                num_classes: int,\n",
    "                hidden_sizes=(64, 32),\n",
    "                activation=\"relu\",\n",
    "                dropout=0.0,\n",
    "                use_batchnorm=False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build an MLP for multiclass classification.\n",
    "    - Output layer: Linear(last_hidden, num_classes)  (NO Sigmoid/Softmax here)\n",
    "    - activation: 'relu' | 'leakyrelu' | 'elu'\n",
    "    - optional: Dropout, BatchNorm\n",
    "    \"\"\"\n",
    "\n",
    "    act_map = {\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"leakyrelu\": lambda: nn.LeakyReLU(negative_slope=0.01),\n",
    "        \"elu\": nn.ELU,\n",
    "    }\n",
    "    if activation not in act_map:\n",
    "        raise ValueError(f\"Unknown activation='{activation}'. Choose from {list(act_map.keys())}.\")\n",
    "\n",
    "    layers = []\n",
    "    prev = input_dim\n",
    "\n",
    "    for h in hidden_sizes:\n",
    "        layers.append(nn.Linear(prev, h))\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(h))\n",
    "        layers.append(act_map[activation]())\n",
    "        if dropout and dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        prev = h\n",
    "\n",
    "    layers.append(nn.Linear(prev, num_classes))  # logits\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train / Eval (epoch-level)\n",
    "# -----------------------------\n",
    "def train_one_epoch(model: nn.Module, train_loader: DataLoader, criterion, optimizer, device=\"cpu\") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return float(total_loss / num_batches)\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model: nn.Module, val_loader: DataLoader, device=\"cpu\") -> float:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(X_batch)\n",
    "            preds = torch.argmax(logits, dim=1)  # class index\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    return float(correct / total)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Full experiment runner (compare architectures)\n",
    "# -----------------------------\n",
    "def run_experiment(X_train, y_train, X_val, y_val,\n",
    "                   num_classes: int,\n",
    "                   config: dict,\n",
    "                   epochs=20,\n",
    "                   batch_size=256,\n",
    "                   lr=1e-3,\n",
    "                   weight_decay=0.0,\n",
    "                   device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Train for N epochs with one architecture config and return best val accuracy.\n",
    "    \"\"\"\n",
    "    train_ds = MaintenanceDataset(X_train, y_train)\n",
    "    val_ds = MaintenanceDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = build_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        num_classes=num_classes,\n",
    "        hidden_sizes=config.get(\"hidden_sizes\", (64, 32)),\n",
    "        activation=config.get(\"activation\", \"relu\"),\n",
    "        dropout=config.get(\"dropout\", 0.0),\n",
    "        use_batchnorm=config.get(\"use_batchnorm\", False),\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device=device)\n",
    "        val_acc = evaluate_accuracy(model, val_loader, device=device)\n",
    "\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        history.append((epoch, train_loss, val_acc))\n",
    "\n",
    "    return model, best_val_acc, history\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Example usage: load CSV + compare multiple architectures\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"predictive_maintenance.csv\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val, preprocessor, scaler, label_enc = prepare_failure_type_data(df)\n",
    "    num_classes = len(label_enc.classes_)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Define architectures to compare\n",
    "    configs = [\n",
    "        {\"name\": \"MLP_ReLU_64-32\", \"hidden_sizes\": (64, 32), \"activation\": \"relu\", \"dropout\": 0.0, \"use_batchnorm\": False},\n",
    "        {\"name\": \"MLP_ReLU_128-64-32 + Dropout\", \"hidden_sizes\": (128, 64, 32), \"activation\": \"relu\", \"dropout\": 0.2, \"use_batchnorm\": False},\n",
    "        {\"name\": \"MLP_LeakyReLU_64-32 + BN\", \"hidden_sizes\": (64, 32), \"activation\": \"leakyrelu\", \"dropout\": 0.0, \"use_batchnorm\": True},\n",
    "        {\"name\": \"MLP_ELU_128-64 + Dropout+BN\", \"hidden_sizes\": (128, 64), \"activation\": \"elu\", \"dropout\": 0.3, \"use_batchnorm\": True},\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    best_overall = None\n",
    "\n",
    "    for cfg in configs:\n",
    "        model, best_val_acc, history = run_experiment(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            num_classes=num_classes,\n",
    "            config=cfg,\n",
    "            epochs=25,\n",
    "            batch_size=256,\n",
    "            lr=1e-3,\n",
    "            weight_decay=1e-4,   # L2 regularization\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        results.append((cfg[\"name\"], best_val_acc))\n",
    "        if best_overall is None or best_val_acc > best_overall[1]:\n",
    "            best_overall = (cfg[\"name\"], best_val_acc, model)\n",
    "\n",
    "        print(f\"{cfg['name']}: best val acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClasses (Failure Type):\")\n",
    "    print(list(label_enc.classes_))\n",
    "\n",
    "    print(\"\\nBest architecture:\")\n",
    "    print(f\"{best_overall[0]} with best val acc = {best_overall[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b4985-41a4-44a4-a0c7-77ca84d1245d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
